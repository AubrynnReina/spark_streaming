{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4702173e-7730-441f-9c88-319f66f7ec8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refactor the code to class and functions for testing\n",
    "class StreamWordCounts():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.DATA_DIR = 'dbfs:/FileStore/test_data/text'\n",
    "        self.CHECKPOINT_DIR = 'dbfs:/FileStore/checkpoint/word_counts'\n",
    "\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        \n",
    "        from pyspark.sql.functions import explode, split\n",
    "        text_data = spark.readStream \\\n",
    "                         .format('text') \\\n",
    "                         .option('lineSep', '.') \\\n",
    "                         .load(f'{self.DATA_DIR}')\n",
    "\n",
    "        return text_data.select(explode(split(text_data.value, ' ')).alias('word'))\n",
    "    \n",
    "\n",
    "    def get_quality_data(self, raw_words):\n",
    "\n",
    "        from pyspark.sql.functions import lower, trim\n",
    "\n",
    "        return raw_words.select(lower(trim(raw_words.word)).alias('cleaned_words')) \\\n",
    "                        .where('cleaned_words is not null') \\\n",
    "                        .where(\"cleaned_words rlike '[a-z]'\")\n",
    "\n",
    "    def get_word_counts(self, quality_words):\n",
    "\n",
    "        return quality_words.groupBy('cleaned_words').count()\n",
    "\n",
    "\n",
    "    def overwrite_word_counts(self, word_counts):\n",
    "\n",
    "        return word_counts.writeStream \\\n",
    "                          .format('delta') \\\n",
    "                          .option('checkpointLocation', f'{self.CHECKPOINT_DIR}') \\\n",
    "                          .outputMode('complete') \\\n",
    "                          .toTable('word_counts') # Returns a streaming query\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "\n",
    "        print(f'\\tExecuting Word Count...', end='')\n",
    "\n",
    "        raw_words = self.get_raw_data()\n",
    "        quality_words = self.get_quality_data(raw_words)\n",
    "        word_counts = self.get_word_counts(quality_words)\n",
    "        streaming_query = self.overwrite_word_counts(word_counts)\n",
    "\n",
    "        print(' Done.')\n",
    "        return streaming_query\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c58958-b402-4b67-8ce2-13cea8eb817f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class StreamWordCountsTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.BASE_DIR = 'dbfs:/FileStore'\n",
    "        self.DATA_DIR = 'dbfs:/FileStore/test_data/text/'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS word_counts')\n",
    "        dbutils.fs.rm('/user/hive/warehouse/word_counts', recurse=True)\n",
    "        dbutils.fs.rm(f'{self.BASE_DIR}/checkpoint', recurse=True)\n",
    "        dbutils.fs.rm(f'{self.BASE_DIR}/test_data/text/', recurse=True)\n",
    "        dbutils.fs.mkdirs(f'{self.DATA_DIR}')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        print('Getting data...', end='')\n",
    "\n",
    "        dbutils.fs.mkdirs(f'{self.BASE_DIR}/test_data/text/')\n",
    "        dbutils.fs.cp(f'{self.BASE_DIR}/data/text/text_data_{file_num}.txt', \n",
    "                      f'{self.DATA_DIR}')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT SUM(count)\n",
    "            FROM word_counts\n",
    "            WHERE SUBSTR(cleaned_words, 1, 1) == 's'\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "\n",
    "\n",
    "    def run_tests(self):\n",
    "\n",
    "        import time\n",
    "        sleep_time = 10\n",
    "\n",
    "        self.clean_up_for_testing()\n",
    "        word_counter = StreamWordCounts()\n",
    "        streaming_query = word_counter.execute()\n",
    "\n",
    "        expected_results = [25, 32, 37]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            print(f'\\tWaiting for {sleep_time} seconds...')\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test completed.\\n')\n",
    "\n",
    "        streaming_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25cc3cc-da9f-4caf-b532-b30cd4c9d09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n\tExecuting Word Count... Done.\nTesting file No.1...\nGetting data... Done.\n\tWaiting for 30 seconds...\nFile No.1 test completed.\n\nTesting file No.2...\nGetting data... Done.\n\tWaiting for 30 seconds...\nFile No.2 test completed.\n\nTesting file No.3...\nGetting data... Done.\n\tWaiting for 30 seconds...\nFile No.3 test completed.\n\n"
     ]
    }
   ],
   "source": [
    "stream_word_counts_tester = StreamWordCountsTestSuite()\n",
    "stream_word_counts_tester.run_tests()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming_Solution",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
