{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How state store works in Spark\n",
    "When a late sample of data with a timestamp comes, Spark check for any window in the state store to perform aggregation update for that late sample\n",
    "\n",
    "With watermarking, there is a limit to which Spark can retain the information of past windows:\n",
    "- If the watermarking is at 6 hours, windows which are prior to 6 hours are disposed\n",
    "- And if there is late data from that particular window, Spark just omits that data, there is no update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose the appropriate watermarking\n",
    "- What is the maximum possible delay?\n",
    "- When are late samples not revelant?\n",
    "\n",
    "i.e.1: A dashboard of transaction data, recording transactions from 9 to 17 (8 hours) &rarr; watermarking should be 8 hours\n",
    "\n",
    "i.e.2: That same dashboard, but only requires 99% accuracy for just getting the feel of the transaction flow in the day.\n",
    "\n",
    "You might want to make some analysis on the lateness of each transaction. **How late can we retain the transaction so that we get 99% transaction on the dashboard?** Can be 30 seconds, 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName('Trade Summarizer') \\\n",
    "                                          .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                                          .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeSumary():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "        return StructType([\n",
    "            StructField('created_time', StringType(), nullable=True),\n",
    "            StructField('type', StringType(), nullable=True),\n",
    "            StructField('amount', DoubleType(), nullable=True),\n",
    "            StructField('broker_code', StringType(), nullable=True)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def read_bronze(self):\n",
    "        \n",
    "        return spark.readStream.table('kafka_bz')\n",
    "    \n",
    "\n",
    "    def get_trade(self, kafka_df):\n",
    "\n",
    "        from pyspark.sql.functions import from_json, expr\n",
    "        return kafka_df.select(from_json(kafka_df.value, self.get_schema()).alias('value')) \\\n",
    "                       .select('value.*') \\\n",
    "                       .withColumn('created_time', expr(\"to_timestamp(created_time, 'yyyy-MM-dd HH:mm:ss')\")) \\\n",
    "                       .withColumn('buy', expr(\"CASE WHEN type == 'BUY' THEN amount ELSE 0 END\")) \\\n",
    "                       .withColumn('sell', expr(\"CASE WHEN type == 'SELL' THEN amount ELSE 0 END\"))\n",
    "    \n",
    "\n",
    "    def aggregate_trade(self, trade_df):\n",
    "\n",
    "        from pyspark.sql.functions import window, sum\n",
    "        return trade_df.withWatermark('created_time', '30 minutes') \\\n",
    "                       .groupBy(window(trade_df['created_time'], '15 minutes')) \\\n",
    "                       .agg(\n",
    "                           sum('buy').alias('total_buy'),\n",
    "                           sum('sell').alias('total_sell')\n",
    "                       ) \\\n",
    "                       .select('window.start', 'window.end', 'total_buy', 'total_sell')\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting trade data extracting stream...', end='')\n",
    "\n",
    "        raw_kakfa_df = self.read_bronze()\n",
    "        trade_df = self.get_trade(raw_kakfa_df)\n",
    "        aggregated_df = self.aggregate_trade(trade_df)\n",
    "        streaming_query = aggregated_df.writeStream \\\n",
    "                                       .queryName('trade_summary') \\\n",
    "                                       .format('delta') \\\n",
    "                                       .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/trade_summary') \\\n",
    "                                       .outputMode('complete') \\\n",
    "                                       .toTable('trade_summary')\n",
    "\n",
    "        print(' Done.\\n')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeSummaryTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS kafka_bz')\n",
    "        spark.sql('DROP TABLE IF EXISTS trade_summary')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/kafka_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/kafka_bz')\n",
    "        \n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/trade_summary')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/trade_summary')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/trade_summary')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/trade_summary')\n",
    "\n",
    "        spark.sql('CREATE TABLE kafka_bz(key String, value String) USING delta')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def assert_trade_summary(self, start, end, expected_buy, expected_sell):\n",
    "        \n",
    "        print('\\tStarting Trade Summary validation...', end='')\n",
    "\n",
    "        result = spark.sql(\n",
    "            f'''\n",
    "            SELECT total_buy, total_sell\n",
    "            FROM trade_summary\n",
    "            WHERE date_format(start, 'yyyy-MM-dd HH:mm:ss') = '{start}'\n",
    "            AND date_format(end, 'yyyy-MM-dd HH:mm:ss') = '{end}';\n",
    "            '''\n",
    "        ).collect()\n",
    "\n",
    "        actual_buy = result[0][0]\n",
    "        actual_sell = result[0][1]\n",
    "\n",
    "        assert expected_buy == actual_buy, f'Test failed! Expected buy is {expected_buy}. Got {actual_buy} instead.'\n",
    "        assert expected_sell == actual_sell, f'Test failed! Expected sell is {expected_sell}. Got {actual_sell} instead.'\n",
    "\n",
    "        print(' Done.\\n')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self):\n",
    "\n",
    "        # Sleep time between extract and transform operation\n",
    "        sleep_time = 30  # Only works if sleep_time >= 30\n",
    "        self.clean_up_for_testing()\n",
    "\n",
    "        trade_summary_stream = TradeSumary()\n",
    "        trade_summary_streaming_query = trade_summary_stream.process()\n",
    "\n",
    "        print('Testing first 2 events...')\n",
    "        spark.sql(\n",
    "            '''\n",
    "            INSERT INTO kafka_bz VALUES\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:05:00\", \"type\": \"BUY\", \"amount\": 500, \"broker_code\": \"ABX\"}'),\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:12:00\", \"type\": \"BUY\", \"amount\": 300, \"broker_code\": \"ABX\"}');\n",
    "            '''\n",
    "        )\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_trade_summary('2019-02-05 10:00:00', '2019-02-05 10:15:00', 800, 0)\n",
    "\n",
    "        print('Testing 3rd and 4th events...')\n",
    "        spark.sql(\n",
    "            '''\n",
    "            INSERT INTO kafka_bz VALUES\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:20:00\", \"type\": \"BUY\", \"amount\": 600, \"broker_code\": \"ABX\"}'),\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:40:00\", \"type\": \"BUY\", \"amount\": 900, \"broker_code\": \"ABX\"}');\n",
    "            '''\n",
    "        )\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_trade_summary('2019-02-05 10:15:00', '2019-02-05 10:30:00', 600, 0)\n",
    "        self.assert_trade_summary('2019-02-05 10:30:00', '2019-02-05 10:45:00', 900, 0)\n",
    "\n",
    "        print('Testing late event...')\n",
    "        spark.sql(\n",
    "            '''\n",
    "            INSERT INTO kafka_bz VALUES\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:48:00\", \"type\": \"SELL\", \"amount\": 500, \"broker_code\": \"ABX\"}'),\n",
    "                ('2019-02-05', '{\"created_time\": \"2019-02-05 10:25:00\", \"type\": \"SELL\", \"amount\": 400, \"broker_code\": \"ABX\"}');\n",
    "            '''\n",
    "        )\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_trade_summary('2019-02-05 10:45:00', '2019-02-05 11:00:00', 0, 500)\n",
    "        self.assert_trade_summary('2019-02-05 10:15:00', '2019-02-05 10:30:00', 600, 400)\n",
    "\n",
    "        print('All tests completed.\\n')\n",
    "\n",
    "        trade_summary_streaming_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting trade data extracting stream... Done.\n",
      "\n",
      "Testing first 2 events...\n",
      "\tWaiting for 30 seconds... Done.\n",
      "\tStarting Trade Summary validation... Done.\n",
      "\n",
      "Testing 3rd and 4th events...\n",
      "\tWaiting for 30 seconds... Done.\n",
      "\tStarting Trade Summary validation... Done.\n",
      "\n",
      "\tStarting Trade Summary validation... Done.\n",
      "\n",
      "Testing late event...\n",
      "\tWaiting for 30 seconds... Done.\n",
      "\tStarting Trade Summary validation... Done.\n",
      "\n",
      "\tStarting Trade Summary validation... Done.\n",
      "\n",
      "All tests completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trade_summary_tester = TradeSummaryTestSuite()\n",
    "trade_summary_tester.run_stream_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
