{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumbling vs Sliding Windows\n",
    "Tumbling: fixed-size, non-overlapping\n",
    "\n",
    "Sliding windows: fixed-size, overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName('Trade Summarizer') \\\n",
    "                                          .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                                          .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingAggregate():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "        return StructType([\n",
    "            StructField('created_time', StringType(), nullable=True),\n",
    "            StructField('reading', DoubleType(), nullable=True),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def read_bronze(self):\n",
    "        \n",
    "        return spark.readStream.table('kafka_bz')\n",
    "    \n",
    "\n",
    "    def get_sensor_data(self, kafka_df):\n",
    "\n",
    "        from pyspark.sql.functions import from_json, to_timestamp\n",
    "        return kafka_df.select(\n",
    "                            kafka_df.key.cast('string').alias('sensor_id'),\n",
    "                            from_json(kafka_df.value.cast('string'), self.get_schema()).alias('value')\n",
    "                        ) \\\n",
    "                       .select('sensor_id', 'value.*') \\\n",
    "                       .withColumn('created_time', to_timestamp('created_time', 'yyyy-MM-dd HH:mm:ss'))\n",
    "    \n",
    "\n",
    "    def aggregate_sensor_data(self, sensor_df):\n",
    "\n",
    "        from pyspark.sql.functions import window, max\n",
    "        return sensor_df.withWatermark('created_time', '30 minutes') \\\n",
    "                       .groupBy(\n",
    "                           sensor_df.sensor_id,\n",
    "                           window(sensor_df['created_time'], '15 minutes', '5 minutes')\n",
    "                       ) \\\n",
    "                       .agg(max('reading').alias('max_reading')) \\\n",
    "                       .select('sensor_id', 'window.start', 'window.end', 'max_reading')\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting sensor data extracting stream...', end='')\n",
    "\n",
    "        raw_kakfa_df = self.read_bronze()\n",
    "        trade_df = self.get_sensor_data(raw_kakfa_df)\n",
    "        aggregated_df = self.aggregate_sensor_data(trade_df)\n",
    "        streaming_query = aggregated_df.writeStream \\\n",
    "                                       .queryName('sensor_summary') \\\n",
    "                                       .format('delta') \\\n",
    "                                       .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/sensor_summary') \\\n",
    "                                       .outputMode('complete') \\\n",
    "                                       .toTable('sensor_summary')\n",
    "\n",
    "        print(' Done.\\n')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorSummaryTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/kafka_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/kafka_bz')\n",
    "        \n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/sensor_summary')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/sensor_summary')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS kafka_bz')\n",
    "        spark.sql('DROP TABLE IF EXISTS sensor_summary')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/sensor_summary')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/sensor_summary')\n",
    "\n",
    "        spark.sql('CREATE TABLE kafka_bz(key String, value String) USING delta')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def assert_sensor_summary(self):\n",
    "        \n",
    "        print('\\tStarting Sensor Summary validation...', end='')\n",
    "\n",
    "        actual_result = spark.table('sensor_summary') \\\n",
    "                             .orderBy('start') \\\n",
    "                             .collect()\n",
    "\n",
    "        expected_result = spark.read \\\n",
    "                               .format('csv') \\\n",
    "                               .option('header', 'true') \\\n",
    "                               .load(f'{self.BASE_DIR}/data/results/sliding_window_result.csv') \\\n",
    "                               .orderBy('start') \\\n",
    "                               .collect()\n",
    "\n",
    "        for i, _ in enumerate(actual_result):\n",
    "            \n",
    "            expected = float(expected_result[i]['max_reading'])\n",
    "            actual = actual_result[i]['max_reading']\n",
    "            assert expected == actual, f'Test failed! Expected result is {expected}. Got {actual} instead.'\n",
    "\n",
    "        print(' Done.\\nAll tests passed.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self):\n",
    "\n",
    "        # Sleep time between extract and transform operation\n",
    "        self.clean_up_for_testing()\n",
    "        sleep_time = 5\n",
    "\n",
    "        sensor_summary_stream = SlidingAggregate()\n",
    "        sensor_summary_streaming_query = sensor_summary_stream.process()\n",
    "\n",
    "        print('Testing all events...')\n",
    "        spark.sql(\n",
    "            '''\n",
    "            INSERT INTO kafka_bz VALUES\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 09:54:00\",\"reading\": 36.2}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 09:59:00\",\"reading\": 36.5}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:04:00\",\"reading\": 36.8}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:09:00\",\"reading\": 36.2}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:14:00\",\"reading\": 36.5}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:19:00\",\"reading\": 36.3}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:24:00\",\"reading\": 37.7}'),\n",
    "            ('SET41', '{\"created_time\": \"2019-02-05 10:29:00\",\"reading\": 37.2}')\n",
    "            '''\n",
    "        )\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_sensor_summary()\n",
    "\n",
    "        sensor_summary_streaming_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting sensor data extracting stream... Done.\n",
      "\n",
      "Testing all events...\n",
      "\tWaiting for 5 seconds... Done.\n",
      "\tStarting Sensor Summary validation... Done.\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "sensor_summary_tester = SensorSummaryTestSuite()\n",
    "sensor_summary_tester.run_stream_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
