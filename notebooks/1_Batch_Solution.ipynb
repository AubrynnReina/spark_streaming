{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35893090-a96f-42b8-9edb-a157aab722c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">org.apache.spark.sql.SparkSession@bdf5c87BASE_DIR: String = dbfs:/FileStore\n",
       "DATA_DIR: String = dbfs:/FileStore/text\n",
       "data: org.apache.spark.sql.DataFrame = [value: string]\n",
       "res16: Array[org.apache.spark.sql.Row] = Array([Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine], [ You can express your streaming computation the same way you would express a batch computation on static data], [ The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive], [ You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc], [ The computation is executed on the same optimized Spark SQL engine], [ Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs], [ In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming], [Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees], [ However, since Spark 2], [3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees], [ Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements], [In this guide, we are going to walk you through the programming model and the APIs], [ We are going to explain the concepts mostly using the default micro-batch processing model, and then later discuss Continuous Processing model], [ First, let’s start with a simple example of a Structured Streaming query - a streaming word count])\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">org.apache.spark.sql.SparkSession@bdf5c87BASE_DIR: String = dbfs:/FileStore\nDATA_DIR: String = dbfs:/FileStore/text\ndata: org.apache.spark.sql.DataFrame = [value: string]\nres16: Array[org.apache.spark.sql.Row] = Array([Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine], [ You can express your streaming computation the same way you would express a batch computation on static data], [ The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive], [ You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc], [ The computation is executed on the same optimized Spark SQL engine], [ Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs], [ In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming], [Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees], [ However, since Spark 2], [3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees], [ Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements], [In this guide, we are going to walk you through the programming model and the APIs], [ We are going to explain the concepts mostly using the default micro-batch processing model, and then later discuss Continuous Processing model], [ First, let’s start with a simple example of a Structured Streaming query - a streaming word count])\n</div>",
       "datasetInfos": [
        {
         "name": "data",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "value",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val BASE_DIR = \"dbfs:/FileStore\"\n",
    "val DATA_DIR = \"dbfs:/FileStore/text\"\n",
    "val data = spark.read\n",
    "            .format(\"text\")\n",
    "            .option(\"lineSep\",\".\")\n",
    "            .load(DATA_DIR)\n",
    "\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4702173e-7730-441f-9c88-319f66f7ec8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refactor the code to class and functions for testing\n",
    "class BatchWordCounts():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.DATA_DIR = 'dbfs:/FileStore/test_data/text'\n",
    "\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        \n",
    "        from pyspark.sql.functions import explode, split\n",
    "        text_data = spark.read \\\n",
    "                         .format('text') \\\n",
    "                         .option('lineSep', '.') \\\n",
    "                         .load(f'{self.DATA_DIR}')\n",
    "\n",
    "        return text_data.select(explode(split(text_data.value, ' ')).alias('word'))\n",
    "    \n",
    "\n",
    "    def get_quality_data(self, raw_words):\n",
    "\n",
    "        from pyspark.sql.functions import lower, trim\n",
    "\n",
    "        return raw_words.select(lower(trim(raw_words.word)).alias('cleaned_words')) \\\n",
    "                        .where('cleaned_words is not null') \\\n",
    "                        .where(\"cleaned_words rlike '[a-z]'\")\n",
    "\n",
    "    def get_word_counts(self, quality_words):\n",
    "\n",
    "        return quality_words.groupBy('cleaned_words').count()\n",
    "\n",
    "\n",
    "    def overwrite_word_counts(self, word_counts):\n",
    "\n",
    "        word_counts.write \\\n",
    "                   .format('delta') \\\n",
    "                   .mode('overwrite') \\\n",
    "                   .saveAsTable('word_counts')\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "\n",
    "        print(f'\\tExecuting Word Count...', end='')\n",
    "\n",
    "        raw_words = self.get_raw_data()\n",
    "        quality_words = self.get_quality_data(raw_words)\n",
    "        word_counts = self.get_word_counts(quality_words)\n",
    "        self.overwrite_word_counts(word_counts)\n",
    "\n",
    "        print(' Done.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c58958-b402-4b67-8ce2-13cea8eb817f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BatchWordCountsTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.BASE_DIR = 'dbfs:/FileStore'\n",
    "        self.DATA_DIR = 'dbfs:/FileStore/test_data/text'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS word_counts')\n",
    "        dbutils.fs.rm('/user/hive/warehouse/word_counts', recurse=True)\n",
    "        dbutils.fs.rm(f'{self.BASE_DIR}/checkpoint', recurse=True)\n",
    "        dbutils.fs.rm(f'{self.BASE_DIR}/test_data/text/', recurse=True)\n",
    "        dbutils.fs.mkdirs(f'{self.DATA_DIR}')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        print('Getting data...', end='')\n",
    "\n",
    "        dbutils.fs.mkdirs(f'{self.BASE_DIR}/test_data/text/')\n",
    "        dbutils.fs.cp(f'{self.DATA_DIR}/text_data_{file_num}.txt', \n",
    "                      f'{self.BASE_DIR}/test_data/text/')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT SUM(count)\n",
    "            FROM word_counts\n",
    "            WHERE SUBSTR(cleaned_words, 1, 1) == 's'\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "\n",
    "\n",
    "    def run_tests(self):\n",
    "\n",
    "        self.clean_up_for_testing()\n",
    "        word_counter = BatchWordCounts()\n",
    "\n",
    "        expected_results = [25, 32, 37]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            word_counter.execute()\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test completed.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25cc3cc-da9f-4caf-b532-b30cd4c9d09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Testing file No.1...\n",
      "Getting data... Done.\n",
      "\tExecuting Word Count... Done.\n",
      "+---------------+-----+\n",
      "|  cleaned_words|count|\n",
      "+---------------+-----+\n",
      "|        ensures|    1|\n",
      "|         stream|    2|\n",
      "|fault-tolerance|    1|\n",
      "|           will|    1|\n",
      "|            you|    3|\n",
      "|            can|    2|\n",
      "|          java,|    1|\n",
      "|     guarantees|    1|\n",
      "|         arrive|    1|\n",
      "|         system|    1|\n",
      "|       provides|    1|\n",
      "|            api|    1|\n",
      "|  aggregations,|    1|\n",
      "|             in|    2|\n",
      "|           take|    1|\n",
      "|           same|    2|\n",
      "| fault-tolerant|    1|\n",
      "|      continues|    1|\n",
      "|      scalable,|    1|\n",
      "|          fast,|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "File No.1 test completed.\n",
      "\n",
      "Testing file No.2...\n",
      "Getting data... Done.\n",
      "\tExecuting Word Count... Done.\n",
      "+---------------+-----+\n",
      "|  cleaned_words|count|\n",
      "+---------------+-----+\n",
      "|        ensures|    1|\n",
      "|         stream|    2|\n",
      "|fault-tolerance|    2|\n",
      "|           will|    2|\n",
      "|            you|    4|\n",
      "|            can|    3|\n",
      "|          java,|    1|\n",
      "|     guarantees|    3|\n",
      "|         arrive|    1|\n",
      "|         system|    1|\n",
      "|       provides|    1|\n",
      "|            api|    1|\n",
      "|  aggregations,|    1|\n",
      "|             in|    3|\n",
      "|           take|    1|\n",
      "|           same|    2|\n",
      "| fault-tolerant|    1|\n",
      "|      continues|    1|\n",
      "|      scalable,|    1|\n",
      "|          fast,|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "File No.2 test completed.\n",
      "\n",
      "Testing file No.3...\n",
      "Getting data... Done.\n",
      "\tExecuting Word Count... Done.\n",
      "+---------------+-----+\n",
      "|  cleaned_words|count|\n",
      "+---------------+-----+\n",
      "|        ensures|    1|\n",
      "|         stream|    2|\n",
      "|fault-tolerance|    2|\n",
      "|           will|    2|\n",
      "|            you|    5|\n",
      "|            can|    3|\n",
      "|          java,|    1|\n",
      "|     guarantees|    3|\n",
      "|         arrive|    1|\n",
      "|         system|    1|\n",
      "|       provides|    1|\n",
      "|            api|    1|\n",
      "|  aggregations,|    1|\n",
      "|             in|    4|\n",
      "|           take|    1|\n",
      "|           same|    2|\n",
      "| fault-tolerant|    1|\n",
      "|      continues|    1|\n",
      "|      scalable,|    1|\n",
      "|          fast,|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "File No.3 test completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_word_counts_tester = BatchWordCountsTestSuite()\n",
    "batch_word_counts_tester.run_tests()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Batch_Solution",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
