{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Invoice Reader').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceStreamBatch():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        return '''\n",
    "               InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "               CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "               PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, DeliveryType string,\n",
    "               DeliveryAddress struct<\n",
    "                    AddressLine string,\n",
    "                    City string,\n",
    "                    ContactNumber string,\n",
    "                    PinCode string,\n",
    "                    State string\n",
    "               >,\n",
    "               InvoiceLineItems array<struct<\n",
    "                    ItemCode string,\n",
    "                    ItemDescription string,\n",
    "                    ItemPrice double,\n",
    "                    ItemQty bigint,\n",
    "                    TotalValue double\n",
    "               >>\n",
    "               '''\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .format('json') \\\n",
    "                    .schema(self.get_schema()) \\\n",
    "                    .load(f'{self.BASE_DIR}/test_data/invoices')\n",
    "    \n",
    "\n",
    "    def explode_invoices(self, invoice_df):\n",
    "\n",
    "        return invoice_df.selectExpr(\n",
    "            'InvoiceNumber', 'CreatedTime', 'StoreID', 'PosID',\n",
    "            'CustomerType', 'PaymentMethod', 'DeliveryType',\n",
    "            'DeliveryAddress.City', 'DeliveryAddress.State', 'DeliveryAddress.PinCode',\n",
    "            'explode(InvoiceLineItems) as LineItem'\n",
    "        )\n",
    "    \n",
    "\n",
    "    def flatten_invoices(self, exploded_df):\n",
    "\n",
    "        from pyspark.sql.functions import expr\n",
    "\n",
    "        flattened_df = exploded_df.withColumn('ItemCode', expr('LineItem.ItemCode')) \\\n",
    "                                  .withColumn('ItemDescription', expr('LineItem.ItemDescription')) \\\n",
    "                                  .withColumn('ItemPrice', expr('LineItem.ItemPrice')) \\\n",
    "                                  .withColumn('ItemQty', expr('LineItem.ItemQty')) \\\n",
    "                                  .withColumn('TotalValue', expr('LineItem.TotalValue')) \\\n",
    "                                  .drop('LineItem')\n",
    "        \n",
    "        return flattened_df\n",
    "    \n",
    "\n",
    "    def append_invoices(self, flattened_df, trigger='batch'):\n",
    "        \n",
    "        streaming_query = flattened_df.writeStream \\\n",
    "                                      .format(\"memory\") \\\n",
    "                                      .outputMode('append') \\\n",
    "                                      .option('maxFilePerTrigger', 1)\n",
    "            \n",
    "        if trigger == 'batch':\n",
    "            return streaming_query.trigger(availableNow = True) \\\n",
    "                                  .queryName('invoice_line_items') \\\n",
    "                                  .start(f'{self.BASE_DIR}/stream_tester')\n",
    "\n",
    "        else:\n",
    "            return streaming_query.trigger(processingTime = trigger) \\\n",
    "                                  .queryName('invoice_line_items') \\\n",
    "                                  .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoices') \\\n",
    "                                  .start(f'{self.BASE_DIR}/stream_tester')\n",
    "        \n",
    "\n",
    "    def process(self, trigger='batch'):\n",
    "\n",
    "        print('Starting invoice processing stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        exploded_df = self.explode_invoices(raw_invoice_df)\n",
    "        flattened_df = self.flatten_invoices(exploded_df)\n",
    "        streaming_query = self.append_invoices(flattened_df, trigger=trigger)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamBatchInvoiceTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "        \n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS word_counts')\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoices')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/test_data/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/test_data/invoices')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        print('\\tGetting data...', end='')\n",
    "\n",
    "        shutil.copyfile(src=f'{self.BASE_DIR}/data/invoices/invoices-{file_num}.json', \n",
    "                        dst=f'{self.BASE_DIR}/test_data/invoices/invoices-{file_num}.json')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT COUNT(*)\n",
    "            FROM invoice_line_items\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self, trigger='20 seconds'):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "        invoice_stream = InvoiceStreamBatch()\n",
    "        streaming_query = invoice_stream.process(trigger=trigger)\n",
    "\n",
    "        expected_results = [1253, 2510, 3994]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            self.wait_for_microbatch(sleep_time=sleep_time) # Only works if sleep_time >= 5\n",
    "\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test passed.\\n')\n",
    "\n",
    "        streaming_query.stop()\n",
    "\n",
    "    \n",
    "    def run_batch_tests(self):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "        invoice_stream = InvoiceStreamBatch()\n",
    "\n",
    "        print(f'Testing file 1 and 2...')\n",
    "        self.get_data(1)\n",
    "        self.get_data(2)\n",
    "        invoice_stream.process(trigger='batch')\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_result(2510)\n",
    "        print(f'File 1, 2 test passed.\\n')\n",
    "\n",
    "        print(f'Testing file 3...')\n",
    "        self.get_data(3)\n",
    "        invoice_stream.process(trigger='batch')\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_result(3994)\n",
    "        print(f'File 3 test passed.\\n')           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "Testing file No.1...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.1 test passed.\n",
      "\n",
      "Testing file No.2...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.2 test passed.\n",
      "\n",
      "Testing file No.3...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.3 test passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = StreamBatchInvoiceTestSuite()\n",
    "invoice_stream_tester.run_stream_tests('3 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Testing file 1 and 2...\n",
      "\tGetting data... Done.\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File 1, 2 test passed.\n",
      "\n",
      "Testing file 3...\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File 3 test passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = StreamBatchInvoiceTestSuite()\n",
    "invoice_stream_tester.run_batch_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
