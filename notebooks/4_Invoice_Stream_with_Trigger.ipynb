{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Invoice Reader').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceStreamBatch():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        return '''\n",
    "               InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "               CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "               PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, DeliveryType string,\n",
    "               DeliveryAddress struct<\n",
    "                    AddressLine string,\n",
    "                    City string,\n",
    "                    ContactNumber string,\n",
    "                    PinCode string,\n",
    "                    State string\n",
    "               >,\n",
    "               InvoiceLineItems array<struct<\n",
    "                    ItemCode string,\n",
    "                    ItemDescription string,\n",
    "                    ItemPrice double,\n",
    "                    ItemQty bigint,\n",
    "                    TotalValue double\n",
    "               >>\n",
    "               '''\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .format('json') \\\n",
    "                    .schema(self.get_schema()) \\\n",
    "                    .load(f'{self.BASE_DIR}/test_data/invoices')\n",
    "    \n",
    "\n",
    "    def explode_invoices(self, invoice_df):\n",
    "\n",
    "        return invoice_df.selectExpr(\n",
    "            'InvoiceNumber', 'CreatedTime', 'StoreID', 'PosID',\n",
    "            'CustomerType', 'PaymentMethod', 'DeliveryType',\n",
    "            'DeliveryAddress.City', 'DeliveryAddress.State', 'DeliveryAddress.PinCode',\n",
    "            'explode(InvoiceLineItems) as LineItem'\n",
    "        )\n",
    "    \n",
    "\n",
    "    def flatten_invoices(self, exploded_df):\n",
    "\n",
    "        from pyspark.sql.functions import expr\n",
    "\n",
    "        flattened_df = exploded_df.withColumn('ItemCode', expr('LineItem.ItemCode')) \\\n",
    "                                  .withColumn('ItemDescription', expr('LineItem.ItemDescription')) \\\n",
    "                                  .withColumn('ItemPrice', expr('LineItem.ItemPrice')) \\\n",
    "                                  .withColumn('ItemQty', expr('LineItem.ItemQty')) \\\n",
    "                                  .withColumn('TotalValue', expr('LineItem.TotalValue')) \\\n",
    "                                  .drop('LineItem')\n",
    "        \n",
    "        return flattened_df\n",
    "    \n",
    "\n",
    "    def append_invoices(self, flattened_df, trigger='batch'):\n",
    "        \n",
    "        streaming_query = flattened_df.writeStream \\\n",
    "                                      .format(\"memory\") \\\n",
    "                                      .outputMode('append') \\\n",
    "                                      .option('maxFilePerTrigger', 1)\n",
    "        \n",
    "        if trigger == 'batch':\n",
    "            return streaming_query.trigger(availableNow = True) \\\n",
    "                                  .queryName('invoice_line_items') \\\n",
    "                                  .start(f'{self.BASE_DIR}/stream_tester')\n",
    "\n",
    "        else:\n",
    "            return streaming_query.trigger(processingTime = trigger) \\\n",
    "                                  .queryName('invoice_line_items') \\\n",
    "                                  .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoices') \\\n",
    "                                  .start(f'{self.BASE_DIR}/stream_tester')\n",
    "        \n",
    "\n",
    "    def process(self, trigger='batch'):\n",
    "\n",
    "        print('Starting invoice processing stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        exploded_df = self.explode_invoices(raw_invoice_df)\n",
    "        flattened_df = self.flatten_invoices(exploded_df)\n",
    "        streaming_query = self.append_invoices(flattened_df, trigger=trigger)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamBatchInvoiceTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "        \n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS word_counts')\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoices')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/test_data/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/test_data/invoices')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        print('\\tGetting data...', end='')\n",
    "\n",
    "        shutil.copyfile(src=f'{self.BASE_DIR}/data/invoices/invoices-{file_num}.json', \n",
    "                        dst=f'{self.BASE_DIR}/test_data/invoices/invoices-{file_num}.json')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT COUNT(*)\n",
    "            FROM invoice_line_items\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self, trigger='20 seconds'):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "        invoice_stream = InvoiceStreamBatch()\n",
    "        streaming_query = invoice_stream.process(trigger=trigger)\n",
    "\n",
    "        expected_results = [1253, 2510, 3994]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            self.wait_for_microbatch(sleep_time=sleep_time) # Only works if sleep_time >= 5\n",
    "\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test passed.\\n')\n",
    "\n",
    "        streaming_query.stop()\n",
    "\n",
    "    \n",
    "    def run_batch_tests(self):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "        invoice_stream = InvoiceStreamBatch()\n",
    "\n",
    "        print(f'Testing file 1 and 2...')\n",
    "        self.get_data(1)\n",
    "        self.get_data(2)\n",
    "        invoice_stream.process(trigger='batch')\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_result(2510)\n",
    "        print(f'File 1, 2 test passed.\\n')\n",
    "\n",
    "        print(f'Testing file 3...')\n",
    "        self.get_data(3)\n",
    "        invoice_stream.process(trigger='batch')\n",
    "        self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "        self.assert_result(3994)\n",
    "        print(f'File 3 test passed.\\n')           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "Testing file No.1...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.1 test passed.\n",
      "\n",
      "Testing file No.2...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.2 test passed.\n",
      "\n",
      "Testing file No.3...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.3 test passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = StreamBatchInvoiceTestSuite()\n",
    "invoice_stream_tester.run_stream_tests('3 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Testing file 1 and 2...\n",
      "\tGetting data... Done.\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File 1, 2 test passed.\n",
      "\n",
      "Testing file 3...\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream..."
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "This query does not support recovering from checkpoint location. Delete ../checkpoint/invoices/offsets to start over.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minvoice_stream_tester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_batch_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 99\u001b[0m, in \u001b[0;36mStreamBatchInvoiceTestSuite.run_batch_tests\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting file 3...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[43minvoice_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_microbatch(sleep_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_result(\u001b[38;5;241m3994\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 89\u001b[0m, in \u001b[0;36mInvoiceStreamBatch.process\u001b[1;34m(self, trigger)\u001b[0m\n\u001b[0;32m     87\u001b[0m exploded_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplode_invoices(raw_invoice_df)\n\u001b[0;32m     88\u001b[0m flattened_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_invoices(exploded_df)\n\u001b[1;32m---> 89\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_invoices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m streaming_query\n",
      "Cell \u001b[1;32mIn[22], line 72\u001b[0m, in \u001b[0;36mInvoiceStreamBatch.append_invoices\u001b[1;34m(self, flattened_df, trigger)\u001b[0m\n\u001b[0;32m     65\u001b[0m streaming_query \u001b[38;5;241m=\u001b[39m flattened_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[0;32m     66\u001b[0m                               \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     67\u001b[0m                               \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint/invoices\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m     68\u001b[0m                               \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m     69\u001b[0m                               \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxFilePerTrigger\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trigger \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstreaming_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavailableNow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minvoice_line_items\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/stream_tester\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m streaming_query\u001b[38;5;241m.\u001b[39mtrigger(processingTime \u001b[38;5;241m=\u001b[39m trigger) \\\n\u001b[0;32m     78\u001b[0m                           \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvoice_line_items\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m     79\u001b[0m                           \u001b[38;5;241m.\u001b[39mstart(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/stream_tester\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\sql\\streaming\\readwriter.py:1529\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart())\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: This query does not support recovering from checkpoint location. Delete ../checkpoint/invoices/offsets to start over."
     ]
    }
   ],
   "source": [
    "invoice_stream_tester.run_batch_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
