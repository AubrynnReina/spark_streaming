{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName('Invoice Reader') \\\n",
    "                                          .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                                          .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        return '''\n",
    "               InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "               CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "               PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, DeliveryType string,\n",
    "               DeliveryAddress struct<\n",
    "                    AddressLine string,\n",
    "                    City string,\n",
    "                    ContactNumber string,\n",
    "                    PinCode string,\n",
    "                    State string\n",
    "               >,\n",
    "               InvoiceLineItems array<struct<\n",
    "                    ItemCode string,\n",
    "                    ItemDescription string,\n",
    "                    ItemPrice double,\n",
    "                    ItemQty bigint,\n",
    "                    TotalValue double\n",
    "               >>\n",
    "               '''\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .format('json') \\\n",
    "                    .schema(self.get_schema()) \\\n",
    "                    .option('cleanSource', 'archive') \\\n",
    "                    .option('sourceArchiveDir', f'{self.BASE_DIR}/data/invoices_archive') \\\n",
    "                    .load(f'{self.BASE_DIR}/test_data/invoices')\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting Bronze data extracting stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        streaming_query = raw_invoice_df.writeStream \\\n",
    "                                        .queryName('bronze_ingestion') \\\n",
    "                                        .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoices_bz') \\\n",
    "                                        .outputMode('append') \\\n",
    "                                        .toTable('invoices_bz')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Silver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .table('invoices_bz')\n",
    "\n",
    "\n",
    "    def explode_invoices(self, invoice_df):\n",
    "\n",
    "        return invoice_df.selectExpr(\n",
    "            'InvoiceNumber', 'CreatedTime', 'StoreID', 'PosID',\n",
    "            'CustomerType', 'PaymentMethod', 'DeliveryType',\n",
    "            'DeliveryAddress.City', 'DeliveryAddress.State', 'DeliveryAddress.PinCode',\n",
    "            'explode(InvoiceLineItems) as LineItem'\n",
    "        )\n",
    "\n",
    "\n",
    "    def flatten_invoices(self, exploded_df):\n",
    "\n",
    "        from pyspark.sql.functions import expr\n",
    "\n",
    "        flattened_df = exploded_df\\\n",
    "            .withColumn('ItemCode', expr('LineItem.ItemCode')) \\\n",
    "            .withColumn('ItemDescription', expr('LineItem.ItemDescription')) \\\n",
    "            .withColumn('ItemPrice', expr('LineItem.ItemPrice')) \\\n",
    "            .withColumn('ItemQty', expr('LineItem.ItemQty')) \\\n",
    "            .withColumn('TotalValue', expr('LineItem.TotalValue')) \\\n",
    "            .drop('LineItem')\n",
    "        \n",
    "        return flattened_df\n",
    "\n",
    "\n",
    "    def append_invoices(self, flattened_df):\n",
    "        \n",
    "        return flattened_df.writeStream \\\n",
    "                           .queryName('silver_processing') \\\n",
    "                           .format('delta') \\\n",
    "                           .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoice_line_items') \\\n",
    "                           .outputMode('append') \\\n",
    "                           .toTable('invoice_line_items')\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting Silver processing stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        exploded_df = self.explode_invoices(raw_invoice_df)\n",
    "        flattened_df = self.flatten_invoices(exploded_df)\n",
    "        streaming_query = self.append_invoices(flattened_df)\n",
    "\n",
    "        print(' Done.')\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedalionApproacheTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS invoice_bz')\n",
    "        spark.sql('DROP TABLE IF EXISTS invoice_line_items')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoices_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoices_bz')\n",
    "        \n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoice_line_items')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoice_line_items')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoice_line_items')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoice_line_items')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/test_data/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/test_data/invoices')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        print('\\tGetting data...', end='')\n",
    "\n",
    "        shutil.copyfile(src=f'{self.BASE_DIR}/data/invoices/invoices-{file_num}.json', \n",
    "                        dst=f'{self.BASE_DIR}/test_data/invoices/invoices-{file_num}.json')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT COUNT(*)\n",
    "            FROM invoice_line_items\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "\n",
    "        bronze_extractor = Bronze()\n",
    "        bronze_streaming_query = bronze_extractor.process()\n",
    "\n",
    "        silver_processor = Silver()\n",
    "        silver_streaming_query = silver_processor.process()\n",
    "\n",
    "        expected_results = [1253, 2510, 3994]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            self.wait_for_microbatch(sleep_time=sleep_time) # Only works if sleep_time >= 5\n",
    "\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test passed.\\n')\n",
    "\n",
    "        bronze_streaming_query.stop()\n",
    "        silver_streaming_query.stop()\n",
    "\n",
    "\n",
    "        import os\n",
    "\n",
    "        print('Validating Archive...', end='')\n",
    "        \n",
    "        archive_dir = f'{self.BASE_DIR}/data/invoices_archive'\n",
    "        expected_archive = ['invoices_1.json', 'invoices_2.json']\n",
    "        \n",
    "        scanned_files = [f for f in os.scandir(archive_dir) if f.isfile()]\n",
    "        for f in expected_archive:\n",
    "            assert f in scanned_files, f'Archive Validation failed for {f}.'\n",
    "\n",
    "        print(' Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting Bronze data extracting stream... Done.\n",
      "Starting Silver processing stream... Done.\n",
      "Testing file No.1...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.1 test passed.\n",
      "\n",
      "Testing file No.2...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.2 test passed.\n",
      "\n",
      "Testing file No.3...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File No.3 test passed.\n",
      "\n",
      "Validating Archive... Done.\n"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = MedalionApproacheTestSuite()\n",
    "invoice_stream_tester.run_stream_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
