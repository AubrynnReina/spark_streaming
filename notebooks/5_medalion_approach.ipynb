{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName('Invoice Reader') \\\n",
    "                                          .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                                          .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        return '''\n",
    "               InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "               CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "               PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, DeliveryType string,\n",
    "               DeliveryAddress struct<\n",
    "                    AddressLine string,\n",
    "                    City string,\n",
    "                    ContactNumber string,\n",
    "                    PinCode string,\n",
    "                    State string\n",
    "               >,\n",
    "               InvoiceLineItems array<struct<\n",
    "                    ItemCode string,\n",
    "                    ItemDescription string,\n",
    "                    ItemPrice double,\n",
    "                    ItemQty bigint,\n",
    "                    TotalValue double\n",
    "               >>\n",
    "               '''\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .format('json') \\\n",
    "                    .schema(self.get_schema()) \\\n",
    "                    .option('cleanSource', 'archive') \\\n",
    "                    .option('sourceArchiveDir', f'{self.BASE_DIR}/data/invoices_archive') \\\n",
    "                    .load(f'{self.BASE_DIR}/test_data/invoices')\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting Bronze data extracting stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        streaming_query = raw_invoice_df.writeStream \\\n",
    "                                        .queryName('bronze_ingestion') \\\n",
    "                                        .option('checkpoiontLocation', f'{self.BASE_DIR}/checkpoint/invoices_bz') \\\n",
    "                                        .outputMode('append') \\\n",
    "                                        .toTable('invoices_bz')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream \\\n",
    "                    .table('invoices_bz')\n",
    "\n",
    "\n",
    "    def explode_invoices(self, invoice_df):\n",
    "\n",
    "        return invoice_df.selectExpr(\n",
    "            'InvoiceNumber', 'CreatedTime', 'StoreID', 'PosID',\n",
    "            'CustomerType', 'PaymentMethod', 'DeliveryType',\n",
    "            'DeliveryAddress.City', 'DeliveryAddress.State', 'DeliveryAddress.PinCode',\n",
    "            'explode(InvoiceLineItems) as LineItem'\n",
    "        )\n",
    "\n",
    "\n",
    "    def flatten_invoices(self, exploded_df):\n",
    "\n",
    "        from pyspark.sql.functions import expr\n",
    "\n",
    "        flattened_df = exploded_df\\\n",
    "            .withColumn('ItemCode', expr('LineItem.ItemCode')) \\\n",
    "            .withColumn('ItemDescription', expr('LineItem.ItemDescription')) \\\n",
    "            .withColumn('ItemPrice', expr('LineItem.ItemPrice')) \\\n",
    "            .withColumn('ItemQty', expr('LineItem.ItemQty')) \\\n",
    "            .withColumn('TotalValue', expr('LineItem.TotalValue')) \\\n",
    "            .drop('LineItem')\n",
    "        \n",
    "        return flattened_df\n",
    "\n",
    "\n",
    "    def append_invoices(self, flattened_df):\n",
    "        \n",
    "        return flattened_df.writeStream \\\n",
    "                           .queryName('silver_processing') \\\n",
    "                           .format('delta') \\\n",
    "                           .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoice_line_items') \\\n",
    "                           .outputMode('append') \\\n",
    "                           .toTable('invoice_line_items')\n",
    "\n",
    "\n",
    "    def process(self, trigger='batch'):\n",
    "\n",
    "        print('Starting Silver processing stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        exploded_df = self.explode_invoices(raw_invoice_df)\n",
    "        flattened_df = self.flatten_invoices(exploded_df)\n",
    "        streaming_query = self.append_invoices(flattened_df, trigger=trigger)\n",
    "\n",
    "        print(' Done.')\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedalionApproacheTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS invoice_bz')\n",
    "        spark.sql('DROP TABLE IF EXISTS invoice_line_items')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoice_line_items')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoice_line_items')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/test_data/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/test_data/invoices')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        print('\\tGetting data...', end='')\n",
    "\n",
    "        shutil.copyfile(src=f'{self.BASE_DIR}/data/invoices/invoices-{file_num}.json', \n",
    "                        dst=f'{self.BASE_DIR}/test_data/invoices/invoices-{file_num}.json')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_result(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT COUNT(*)\n",
    "            FROM invoice_line_items\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self):\n",
    "\n",
    "        sleep_time = 10\n",
    "        self.clean_up_for_testing()\n",
    "\n",
    "        bronze_extractor = Bronze()\n",
    "        bronze_streaming_query = bronze_extractor.process()\n",
    "\n",
    "        silver_processor = Silver()\n",
    "        silver_streaming_query = silver_processor.process()\n",
    "\n",
    "        expected_results = [1253, 2510, 3994]\n",
    "        for i in range(len(expected_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            self.wait_for_microbatch(sleep_time=sleep_time) # Only works if sleep_time >= 5\n",
    "\n",
    "            self.assert_result(expected_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test passed.\\n')\n",
    "\n",
    "        bronze_streaming_query.stop()\n",
    "        silver_streaming_query.stop()\n",
    "\n",
    "\n",
    "        import os\n",
    "\n",
    "        print('Validating Archive...', end='')\n",
    "        \n",
    "        archive_dir = f'{self.BASE_DIR}/data/invoices_archive'\n",
    "        expected_archive = ['invoices_1.json', 'invoices_2.json']\n",
    "        \n",
    "        scanned_files = [f for f in os.scandir(archive_dir) if f.isfile()]\n",
    "        for f in scanned_files:\n",
    "            assert f in expected_archive, f'Archive Validation failed for {f}.'\n",
    "\n",
    "        print(' Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning..."
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../checkpoint/invoice_line_item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m invoice_stream_tester \u001b[38;5;241m=\u001b[39m MedalionApproacheTestSuite()\n\u001b[1;32m----> 2\u001b[0m \u001b[43minvoice_stream_tester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stream_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 71\u001b[0m, in \u001b[0;36mMedalionApproacheTestSuite.run_stream_tests\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_stream_tests\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     sleep_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_up_for_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     bronze_extractor \u001b[38;5;241m=\u001b[39m Bronze()\n\u001b[0;32m     74\u001b[0m     bronze_streaming_query \u001b[38;5;241m=\u001b[39m bronze_extractor\u001b[38;5;241m.\u001b[39mprocess()\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mMedalionApproacheTestSuite.clean_up_for_testing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint/invoices_bz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint/invoices_bz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/checkpoint/invoice_line_item\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint/invoice_line_item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test_data/invoices\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\envs\\spark_course\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\envs\\spark_course\\lib\\shutil.py:610\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    608\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\envs\\spark_course\\lib\\shutil.py:607\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    608\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../checkpoint/invoice_line_item'"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = MedalionApproacheTestSuite()\n",
    "invoice_stream_tester.run_stream_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Testing file 1 and 2...\n",
      "\tGetting data... Done.\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File 1, 2 test passed.\n",
      "\n",
      "Testing file 3...\n",
      "\tGetting data... Done.\n",
      "Starting invoice processing stream... Done.\n",
      "\tWaiting for 10 seconds... Done.\n",
      "\tStarting validation... Done.\n",
      "File 3 test passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice_stream_tester = StreamBatchInvoiceTestSuite()\n",
    "invoice_stream_tester.run_batch_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
