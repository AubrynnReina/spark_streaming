{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName('Invoice Reader') \\\n",
    "                                          .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                                          .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "    \n",
    "    def get_schema(self):\n",
    "\n",
    "        return '''\n",
    "               InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "               CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n",
    "               PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, DeliveryType string,\n",
    "               DeliveryAddress struct<\n",
    "                    AddressLine string,\n",
    "                    City string,\n",
    "                    ContactNumber string,\n",
    "                    PinCode string,\n",
    "                    State string\n",
    "               >,\n",
    "               InvoiceLineItems array<struct<\n",
    "                    ItemCode string,\n",
    "                    ItemDescription string,\n",
    "                    ItemPrice double,\n",
    "                    ItemQty bigint,\n",
    "                    TotalValue double\n",
    "               >>\n",
    "               '''\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "        \n",
    "        from pyspark.sql.functions import input_file_name\n",
    "        return spark.readStream \\\n",
    "                    .format('json') \\\n",
    "                    .schema(self.get_schema()) \\\n",
    "                    .load(f'{self.BASE_DIR}/test_data/invoices') \\\n",
    "                    .withColumn('FileName', input_file_name())\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting Bronze data extracting stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        streaming_query = raw_invoice_df.writeStream \\\n",
    "                                        .queryName('bronze_ingestion') \\\n",
    "                                        .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/invoices_bz') \\\n",
    "                                        .outputMode('append') \\\n",
    "                                        .toTable('invoices_bz')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gold():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def read_invoices(self):\n",
    "\n",
    "        return spark.readStream.table('invoices_bz')\n",
    "\n",
    "\n",
    "    def get_points_per_customer(self, invoices_df):\n",
    "\n",
    "        from pyspark.sql.functions import sum, expr\n",
    "\n",
    "        return invoices_df.groupBy('CustomerCardNo') \\\n",
    "                          .agg(\n",
    "                              sum('TotalAmount').alias('TotalAmount'),\n",
    "                              sum(expr('TotalAmount * 0.02')).alias('TotalPoints')\n",
    "                            )\n",
    "\n",
    "\n",
    "    def aggregated_upsert(self, raw_invoice_df, batch_id):\n",
    "\n",
    "        rewards_df = self.get_points_per_customer(raw_invoice_df)\n",
    "        rewards_df.createOrReplaceTempView('customer_rewards_temp_view')\n",
    "        merge_statement = '''\n",
    "            MERGE INTO customer_rewards t\n",
    "            USING customer_rewards_temp_view s\n",
    "            ON s.CustomerCardNo = t.CustomerCardNo\n",
    "            WHEN MATCHED THEN\n",
    "            UPDATE SET t.TotalAmount = t.TotalAmount + s.TotalAmount,\n",
    "                       t.TotalPoints = t.TotalPoints + s.TotalPoints\n",
    "            WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "        '''\n",
    "        \n",
    "        rewards_df._jdf.sparkSession().sql(merge_statement)\n",
    "\n",
    "    def save_aggregation(self, aggregated_df):\n",
    "        \n",
    "        return aggregated_df.writeStream \\\n",
    "                            .queryName('gold_update') \\\n",
    "                            .format('delta') \\\n",
    "                            .option('checkpointLocation', f'{self.BASE_DIR}/checkpoint/customer_rewards') \\\n",
    "                            .outputMode('update') \\\n",
    "                            .foreachBatch(self.aggregated_upsert) \\\n",
    "                            .start()\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        print('Starting Gold updating stream...', end='')\n",
    "\n",
    "        raw_invoice_df = self.read_invoices()\n",
    "        streaming_query = self.save_aggregation(raw_invoice_df)\n",
    "\n",
    "        print(' Done.\\n')\n",
    "        return streaming_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.BASE_DIR = '..'\n",
    "\n",
    "\n",
    "    def clean_up_for_testing(self):\n",
    "\n",
    "        import shutil\n",
    "        import os\n",
    "\n",
    "        print('Starting cleaning...', end='')\n",
    "\n",
    "        spark.sql('DROP TABLE IF EXISTS invoice_bz')\n",
    "        spark.sql('DROP TABLE IF EXISTS customer_rewards')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoices_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/invoices_bz')\n",
    "        \n",
    "        shutil.rmtree(f'{self.BASE_DIR}/notebooks/spark-warehouse/customer_rewards')\n",
    "        os.makedirs(f'{self.BASE_DIR}/notebooks/spark-warehouse/customer_rewards')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/invoices_bz')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/checkpoint/customer_rewards')\n",
    "        os.makedirs(f'{self.BASE_DIR}/checkpoint/customer_rewards')\n",
    "\n",
    "        shutil.rmtree(f'{self.BASE_DIR}/test_data/invoices')\n",
    "        os.makedirs(f'{self.BASE_DIR}/test_data/invoices')\n",
    "\n",
    "        spark.sql('CREATE TABLE customer_rewards(CustomerCardNo STRING, TotalAmount DOUBLE, TotalPoints DOUBLE) USING delta')\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def get_data(self, file_num):\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        print('\\tGetting data...', end='')\n",
    "\n",
    "        shutil.copyfile(src=f'{self.BASE_DIR}/data/invoices/invoices-{file_num}.json', \n",
    "                        dst=f'{self.BASE_DIR}/test_data/invoices/invoices-{file_num}.json')\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "    \n",
    "    def assert_bronze(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting Bronze validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT COUNT(*)\n",
    "            FROM invoices_bz\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'\n",
    "        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def assert_gold(self, expected_result):\n",
    "        \n",
    "        print('\\tStarting Gold validation...', end='')\n",
    "\n",
    "        actual_result = spark.sql(\n",
    "            '''\n",
    "            SELECT TotalAmount\n",
    "            FROM customer_rewards\n",
    "            WHERE CustomerCardNo = '2262471989'\n",
    "            '''\n",
    "        ).collect()[0][0]\n",
    "\n",
    "        assert expected_result == actual_result, f'Test failed! Expected result is {expected_result}. Got {actual_result} instead.'        \n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def wait_for_microbatch(self, sleep_time=15):\n",
    "\n",
    "        import time\n",
    "\n",
    "        print(f'\\tWaiting for {sleep_time} seconds...', end='')\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        print(' Done.')\n",
    "\n",
    "\n",
    "    def run_stream_tests(self):\n",
    "\n",
    "        # Sleep time between extract and transform operation\n",
    "        sleep_time = 20  # Only works if sleep_time >= 20\n",
    "        self.clean_up_for_testing()\n",
    "\n",
    "        bronze_extractor = Bronze()\n",
    "        bronze_streaming_query = bronze_extractor.process()\n",
    "\n",
    "        gold_update = Gold()\n",
    "        gold_streaming_query = gold_update.process()\n",
    "\n",
    "        expected_bronze_results = [501, 501 + 500, 501 + 500 + 590]\n",
    "        expected_gold_results = [36859, 36859 + 20740, 36859 + 20740 + 31959]\n",
    "\n",
    "        for i in range(len(expected_bronze_results)):\n",
    "\n",
    "            print(f'Testing file No.{i + 1}...')\n",
    "\n",
    "            self.get_data(i + 1)\n",
    "            self.wait_for_microbatch(sleep_time=sleep_time)\n",
    "\n",
    "            self.assert_bronze(expected_bronze_results[i])\n",
    "            self.assert_gold(expected_gold_results[i])\n",
    "\n",
    "            print(f'File No.{i + 1} test passed.\\n')\n",
    "\n",
    "        bronze_streaming_query.stop()\n",
    "        gold_streaming_query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning... Done.\n",
      "Starting Bronze data extracting stream... Done.\n",
      "Starting Gold updating stream... Done.\n",
      "\n",
      "Testing file No.1...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 20 seconds... Done.\n",
      "\tStarting Bronze validation... Done.\n",
      "\tStarting Gold validation... Done.\n",
      "File No.1 test passed.\n",
      "\n",
      "Testing file No.2...\n",
      "\tGetting data... Done.\n",
      "\tWaiting for 20 seconds..."
     ]
    }
   ],
   "source": [
    "aggregated_stream_tester = AggregationTestSuite()\n",
    "aggregated_stream_tester.run_stream_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
